sapply(levels(factor(data.full$Parch)), function(f){ mean(subset(data.full, Parch == f)$Survived) } )
data.full$ParchF = '0'
data.full[data.full$Parch >=1, 'ParchF'] = '1'
data.full$ParchF = factor(data.full$ParchF)
summary(factor(data.full$ParchF))
sapply(levels(factor(data.full$ParchF)), function(f){ mean(subset(data.full, ParchF == f)$Survived) } )
##############################
# scale fare
data.full$FareModif = data.full$Fare/100
hist(data.full$Fare)
data.full$fareCateg = factor(floor(data.full$Fare/25))
levels(fareCateg)
summary(data.full$fareCateg)
prob = sapply(levels(data.full$fareCateg), function(f){ mean(subset(data.full, fareCateg == f)$Survived) } )
x = as.numeric(attributes(prob)$names) * 25
plot(prob ~ x)
plot(Survived ~ Fare, data.full)
##########################################
# Embarked
summary(data.full$Embarked)
sapply(levels(factor(data.full$Embarked)), function(f){ mean(subset(data.full, Embarked == f)$Survived) } )
########################################
# Fare
hist(data.full$Fare, main='Histogram of Fare', xlab='Fare')
#############################################
#### sub-sample
set.seed(22021991)
learning.sample = sample(1:nrow(data.full), 600,replace=FALSE)
data = data.full[learning.sample,]
validation = data.full[-learning.sample,]
######################################
# linear model
m = glm(Survived ~ Sex + AgeF + Pclass + SibSpF + ParchF + FareModif + Embarked, data, family='binomial')
summary(m)
# validation in frequentist approach
predictions = predict(m, validation, type='response')
predictions.01 = rep(0,length(predictions))
predictions.01[predictions > 0.5] = 1
summary(predictions.01 == validation$Survived)
summary(predictions.01[validation$Survived==1] == validation[validation$Survived==1,]$Survived)
summary(predictions.01[validation$Survived==0] == validation[validation$Survived==0,]$Survived)
######################################
### model matrix
fTransformDatum = function(datum){
c( 1, datum$Sex == 'male', datum$AgeF == '>=45',  datum$AgeF == '14:30', datum$AgeF == '30:45', datum$AgeF == 'NA', datum$Pclass, datum$SibSpF == '1', datum$SibSpF == '2+', datum$ParchF == '1', datum$FareModif, datum$Embarked == 'Q', datum$Embarked == 'S')
}
response = data$Survived
response.val = validation$Survived
model.matrix = matrix(0, ncol = 13, nrow=nrow(data))
for (i in 1:nrow(model.matrix)){
model.matrix[i,] = fTransformDatum(data[i,])
}
model.matrix.val = matrix(0, ncol = 13, nrow=nrow(validation))
for (i in 1:nrow(model.matrix.val)){
model.matrix.val[i,] = fTransformDatum(validation[i,])
}
########################################################################################################################
########################################################################################################################
########################################################################################################################
#######                Bayes fit - gaussian prior
prior.sigma = 20
fLogPosteriorUnsc = function(theta){
response %*% model.matrix %*% theta - sum( log(1+exp(model.matrix%*%theta)) ) - 1/prior.sigma^2 * t(theta) %*% theta
}
###############################################
# find MAP
fLogPosteriorGradient = function(theta){
response %*% model.matrix-apply(apply(model.matrix, 1, function(xi){ (exp(t(xi)%*%theta)*xi)/( 1+exp(t(xi)%*%theta) ) } ),1,sum)-2*theta/prior.sigma^2
}
lambda = .001
theta = rep(0,13)
for (i in 1:10000){
theta = as.vector(theta + lambda * fLogPosteriorGradient(theta))
}
theta.map = theta
theta.map
fLogPosteriorGradient(theta)
plot(m$coefficients, col='red', pch=19)
points(theta, col='blue', pch=19)
###############################################
### Laplace approximation
hessian.map = -2/prior.sigma^2 * diag(13)
for(i in 1:nrow(model.matrix) ){
xi = model.matrix[i,]
hessian.map = hessian.map - as.numeric(exp(xi%*%theta.map)/(1+exp(xi%*%theta.map))^2 ) *  xi %*% t(xi)
}
laplace.covariance = ginv(-hessian.map)
diag(laplace.covariance)
# prediction
predictions = rep(NA, nrow(validation))
for(i in 1:nrow(validation)){
xi = fTransformDatum(validation[i,])
theta.monte.carlo = mvrnorm(n = 1000, theta.map, laplace.covariance)
predictions[i]=mean(as.vector(exp(xi%*% t(theta.monte.carlo))/(1+exp(xi%*%t(theta.monte.carlo)))))
}
hist(predictions)
# validation in Laplace approximation
predictions.01 = rep(0,length(predictions))
predictions.01[predictions > 0.5] = 1
summary(predictions.01 == validation$Survived)
summary(predictions.01[validation$Survived==1] == validation[validation$Survived==1,]$Survived)
summary(predictions.01[validation$Survived==0] == validation[validation$Survived==0,]$Survived)
####
# log-likelihood of data
response %*% model.matrix %*% theta.map - sum( log(1+exp(model.matrix%*%theta.map)) )
response.val %*% model.matrix.val %*% theta.map - sum( log(1+exp(model.matrix.val%*%theta.map)) )
#
#
#
# ########################################################################################################
# ########################################################################################################
# # Expectation-Propagation approximation
#
# # note: the dimension of natural parameter is 13*13 + 13 = ...
#
# fMuSigma2Natural = function(mu,sigma){
#   inv.sigma = ginv(sigma)
#   return(c( as.vector(inv.sigma), inv.sigma %*% mu  ))
# }
# fNatural2MuSigma = function(natural){
#   inv.sigma = matrix( natural[1:196], nrow=13 )
#   sigma = ginv(inv.sigma)
#   mu = sigma %*% natural[197:210]
#   return(list(sigma,mu))
# }
#
#
# # variable for storing natural parameters
# natural = matrix(0, ncol=210, nrow= nrow(model.matrix)+1 ) # there is one more for the prior
# for (i in 1:nrow(natural)){
#   natural[i,1:196] = as.vector(diag(13)) # start with identity covariance matrix
# }
#
# matrix(natural[1,1:196], nrow=13)
# i=1
#
# # EP loop
# for (k in 1:1){
#
#   ##############################
#   # logistic factors
#   # for (i in 1:nrow(model.matrix)){
#   for (i in 1:295){
#
#     # features of the i-th factor
#     xi = model.matrix[i,] # the i-th row of the model matrix
#     yi = response[i] # the y_i
#
#     # parameters of the cavity
#     lambda.sum.wo.i = apply(natural, 2, sum) - natural[i,] # natural parameters
#     pars = fNatural2MuSigma(lambda.sum.wo.i)
#     mu = pars[[2]] # corresponding mean
#     sigma = pars[[1]] # corresponding variance
#
#     # projection onto xi
#     proj.xi.mu = as.numeric(xi %*% mu)
#     proj.xi.sigma2 = as.numeric(t(xi) %*% sigma %*% xi)
#
#     # transform back to natural parameters
#     proj.xi.b = 1/proj.xi.sigma2
#     proj.xi.r = 1/proj.xi.sigma2 * proj.xi.mu
#
#     # calculate the Laplace transformation for the hybrid - i.e. maximum density point and the second derivative (of log) there
#     a = proj.xi.mu # initial point for gradient ascend
#     lambda = 5e-04
#     for (k.ga in 1:1000){ # gradient ascend finds the point with the highest density
#       a=a + lambda*( -exp(a)/(1+exp(a)) - (a-proj.xi.mu)/(proj.xi.sigma2) + yi )
#     }
#     grid.center = a
#     grid.sigma2 = -1/( (1/exp(2*a) / (1+exp(a))^2 - exp(a)/(1+exp(a)) - 1/proj.xi.sigma2) )
#
#     # calculate mean and variance of the hybrid
#     grid = (seq(-1,1, length.out = 1000) + grid.center) * sqrt(grid.sigma2) * 5
#     hybrid = sapply( grid, function(alpha){exp(yi*alpha)/(1+exp(alpha)) * exp( -(alpha-proj.xi.mu)^2/(2*proj.xi.sigma2) )} )
#     hybrid.mean = sum(hybrid * grid) / sum(hybrid)
#     hybrid.var = sum(hybrid * grid^2) / sum(hybrid) - hybrid.mean^2
#
#     # convert hybrid mean and variance into natural parameters of 1-dim normal distribution
#     hybrid.b = 1/hybrid.var
#     hybrid.r = 1/hybrid.var * hybrid.mean
#
#     # coresponding parameters of the factor
#     factor.proj.xi.a = hybrid.r - proj.xi.r
#     factor.proj.xi.b = hybrid.b - proj.xi.b
#     factor.r = factor.proj.xi.a * xi
#     factor.b = as.numeric(factor.proj.xi.b) * as.vector(xi %*% t(xi))
#
#     # save the guy
#     natural[i,] = c(factor.b, factor.r)
#   }
#
#
#
#
#   # prior factor
# }
#
###############################################################################################
###############################################################################################
#################### Metropolis Hasting chain
theta = mvrnorm( mu=theta.map, Sigma=laplace.covariance )
sigma = .3
n.loops = 1000000
trajectory = matrix(NA, ncol = 13, nrow=n.loops)
trajectory[1,] = theta
num.of.accept = 0
for(i in 1:n.loops){
proposal = mvrnorm( mu=theta, Sigma=sigma*laplace.covariance )
p.theta = as.numeric(exp(fLogPosteriorUnsc(theta)))
p.proposal = as.numeric(exp(fLogPosteriorUnsc(proposal)))
p.accept = min(1, p.proposal/p.theta)
if (runif(1) < p.accept){
theta = proposal
trajectory[num.of.accept+1,] = theta
num.of.accept = num.of.accept + 1
}
}
# acceptance rate
num.of.accept/n.loops
# what I take as my sample - the second half
theta.sampling = trajectory[ floor(num.of.accept/2):num.of.accept, ]
# prediction
predictions = rep(NA, nrow(validation))
for(i in 1:nrow(validation)){
xi = fTransformDatum(validation[i,])
predictions[i]=mean(as.vector(exp(xi%*% t(theta.sampling))/(1+exp(xi%*%t(theta.sampling)))))
}
# validation in MH sampling
predictions.01 = rep(0,length(predictions))
predictions.01[predictions > 0.5] = 1
summary(predictions.01 == validation$Survived)
summary(predictions.01[validation$Survived==1] == validation[validation$Survived==1,]$Survived)
summary(predictions.01[validation$Survived==0] == validation[validation$Survived==0,]$Survived)
########################################################################################################################
########################################################################################################################
#############################                   VARIABLE SELECTION                 #####################################
########################################################################################################################
########################################################################################################################
number2binary = function(number, noBits) {
binary_vector = rev(as.numeric(intToBits(number)))
if(missing(noBits)) {
return(binary_vector)
} else {
binary_vector[-(1:(length(binary_vector) - noBits))]
}
}
number2binary(128,7)
combinations = t(sapply(0:127, number2binary, noBits=7))
combinations.likelihood = rep(NA, 128)
# what columns are active given a combination
what.columns = function(comb){
cols = c(1) # intercept
if (comb[1] == 1) { cols = c(cols,2) } # sex
if (comb[2] == 1) { cols = c(cols,3,4,5,6)} # age
if (comb[3] == 1) { cols = c(cols,7) } # ticket class
if (comb[4] == 1) { cols = c(cols,8,9) } # siblings
if (comb[5] == 1) { cols = c(cols,10) } # parch
if (comb[6] == 1) { cols = c(cols,11) } # fare
if (comb[7] == 1) { cols = c(cols,12,13) } # embarked
return(cols)
}
model.matrix.comb = lapply(1:128, function(i){model.matrix[,what.columns(combinations[i,])] } )
#############################################################
# find MAP for each combination
prior.sigma = 20
# some useful functions
fLogPosteriorUnsc.Comb = function(theta,comb.i){
if (comb.i == 1){
sum(response) * theta - length(response) * log(1+exp(theta)) - 1/prior.sigma^2 * theta^2
} else{
response %*% model.matrix.comb[[comb.i]] %*% theta - sum( log(1+exp(model.matrix.comb[[comb.i]]%*%theta)) ) - 1/prior.sigma^2 * t(theta) %*% theta
}
}
fLogPosteriorGradient.Comb = function(theta,comb.i){
if (comb.i == 1){
sum(response) - length(response)*exp(theta)/(1+exp(theta)) - 2/prior.sigma^2 * theta
} else {
response %*% model.matrix.comb[[comb.i]]-apply(apply(model.matrix.comb[[comb.i]], 1, function(xi){ (exp(t(xi)%*%theta)*xi)/( 1+exp(t(xi)%*%theta) ) } ),1,sum)-2*theta/prior.sigma^2
}
}
# this functions finds MAP for given combination number i
fFindMAP.Comb = function(comb.i){
# print(paste('find MAP for comb.i = ',comb.i,paste=''))
num.iterations = 10000
lambda = .001 # step size
num.pars = length(what.columns(combinations[comb.i,])) # how many parameters, dimension of theta
theta = rep(0, num.pars) # initial point
for (j in 1:num.iterations){
theta = as.vector(theta + lambda * fLogPosteriorGradient.Comb(theta,comb.i))
}
return( theta )
# return( list(comb.i,theta) )
}
# loop for finding MAP for each combination
clusterExport(cl=cl, varlist=c('response','model.matrix.comb','prior.sigma','what.columns','combinations','fLogPosteriorGradient.Comb' )) # global variables must be assigned to the cluster
combinations.map = parLapply(cl, 1:128, fFindMAP.Comb )
# combinations[128,]
# combinations.map[[128]]
# theta.map
# fLogPosteriorGradient.Comb( combinations.map[[80]], 80)
# combinations.map
# fFindMAP.Comb(1)
################################################
### Laplace approx
fLaplaceCov.Comb = function(comb.i){
if (comb.i == 1){
theta.map = combinations.map[[1]]
hessian.map = -length(response) * (exp(theta)*(1+exp(theta))-exp(2*theta)) / (1+exp(theta))^2 - 2/prior.sigma^2
return( -1/hessian.map )
} else {
num.pars = length(what.columns(combinations[comb.i,])) # how many parameters, dimension of theta
theta.map = combinations.map[[comb.i]] # already calculated MAP
# calculate the Hessian
hessian.map = -2/prior.sigma^2 * diag(num.pars)
for(i in 1:nrow(model.matrix) ){
xi = model.matrix.comb[[comb.i]][i,]
hessian.map = hessian.map - as.numeric(exp(xi%*%theta.map)/(1+exp(xi%*%theta.map))^2 ) *  xi %*% t(xi)
}
return(ginv(-hessian.map))
}
}
# loop for finding Hessian for each combination
combinations.laplaceCov = lapply( 1:128, fLaplaceCov.Comb )
###############################################
### calculate the likelihood for each comb
fNormalizingConstant.Comb = function(comb.i){
num.pars = length(what.columns(combinations[comb.i,])) # how many parameters, dimension of theta
theta.map = combinations.map[[comb.i]]
covariance = combinations.laplaceCov[[comb.i]]
if (comb.i==1){
theta.prior.sample = rnorm(n=10000, mean=0, sd=prior.sigma) # sampling from prior
mean(exp( sum(response) * theta.prior.sample - length(response)*( log(1+exp(theta.prior.sample)) )))
} else {
num.pars = length(what.columns(combinations[comb.i,])) # how many parameters, dimension of theta
theta.post.sample = mvrnorm(n=10000, mu=combinations.map[[comb.i]] , Sigma= combinations.laplaceCov[[comb.i]] )
# sampling = t( # conditional model
#        exp(response %*% model.matrix.comb[[comb.i]] %*% t(theta.post.sample) - apply( log(1+exp(model.matrix.comb[[comb.i]]%*%t(theta.post.sample))),2,sum) )
#       # multiply by prior
#        * apply( theta.post.sample, 1, function(theta){( (2*pi)^(-1/2*num.pars)*( (prior.sigma^2)^num.pars )^(-1/2)*exp( -1/2*t(theta)%*% (prior.sigma^(-2) * diag(num.pars)) %*%(theta)))  } )
#       # divide by Laplace approx
#        / apply( theta.post.sample, 1, function(theta){( (2*pi)^(-1/2*num.pars)*det(covariance)^(-1/2)*exp( -1/2*t(theta-theta.map)%*%ginv(covariance)%*%(theta-theta.map)))  } )
#           )
# return( mean(sampling) )
likelihood = 0
for (k in 1:nrow(theta.post.sample)){
theta = theta.post.sample[k,]
likelihood = likelihood + exp( (response %*% model.matrix.comb[[comb.i]]) %*% theta ) / prod( 1 + exp( model.matrix.comb[[comb.i]] %*% theta)) / ( (2*pi)^(-1/2*num.pars)*( (prior.sigma^2)^num.pars )^(-1/2)*exp( -1/2*t(theta)%*% (prior.sigma^(-2) * diag(num.pars)) %*%(theta)))  * ( (2*pi)^(-1/2*num.pars)*det(covariance)^(-1/2)*exp( -1/2*t(theta-theta.map)%*%ginv(covariance)%*%(theta-theta.map)))
}
return(likelihood/nrow(theta.post.sample))
}
}
clusterExport(cl=cl, varlist=c('combinations.map','combinations.laplaceCov','mvrnorm','ginv' )) # global variables must be assigned to the cluster
combinations.NormalizingConstant = parSapply(cl, 1:128, fNormalizingConstant.Comb )
#combinations.NormalizingConstant = sapply(1:128, fNormalizingConstant.Comb)
combinations[83,]
combinations.NormalizingConstant[83]
which.max(combinations.NormalizingConstant)
combinations[which.max(combinations.NormalizingConstant),]
###############################################
### weight by prior
fPrior.Comb = function(comb.i){
num.pars = sum(combinations[comb.i,]) # how many parameters
return( 0.05^num.pars*0.95^(7-num.pars))
}
combinations.Prior = sapply(1:128, fPrior.Comb)
combinations.PosteriorUnscaled = combinations.NormalizingConstant * combinations.Prior
which.max(combinations.PosteriorUnscaled)
combinations[which.max(combinations.PosteriorUnscaled),]
a = 1/sum(combinations.PosteriorUnscaled)
combinations.Posterior = combinations.PosteriorUnscaled * a
combinations.Posterior
which.max(combinations.Posterior)
combinations.Posterior[which.max(combinations.Posterior)]
# what is the first, second, third most likely (by posterior) model
first.models = order(combinations.Posterior, decreasing = T)[1:10]
combinations.Posterior[first.models]
combinations[first.models,]
#############################################
### prediction
sum.of.probabilities = sum(combinations.Posterior[first.models]) # how much probability is explained by first 10 models - normalazing constant
what.columns(combinations[100,])
predictions = rep(NA, nrow(validation))
for(i in 1:nrow(validation)){
xi = fTransformDatum(validation[i,])
pred = 0
for(k in first.models){
xi.subv = xi[what.columns(combinations[k,])]
theta.monte.carlo = mvrnorm(n = 1000, combinations.map[[k]], combinations.laplaceCov[[k]])
pred = pred + mean(as.vector(exp(xi.subv%*% t(theta.monte.carlo))/(1+exp(xi.subv%*%t(theta.monte.carlo))))) * combinations.Posterior[k]/sum.of.probabilities
}
predictions[i]= pred
}
hist(predictions)
# validation
predictions.01 = rep(0,length(predictions))
predictions.01[predictions > 0.5] = 1
summary(predictions.01 == validation$Survived)
data(package="spacetime")
install.packages('spacetime')
library(spacetime)
R.version
# installing/loading the package:
if(!require(installr)) {
install.packages("installr"); require(installr)} #load / install+load installr
# using the package:
updateR() # this will start the updating process of your R installation.  It will check for newer versions, and if one is available, will guide you through the decisions you'd need to make.
R.version
R.version
data(package="spacetime")
air
data(air)
data(package="spacetime")
data(fires)
library(spacetime)
data(air)
str(air)
head(air)
plot(air)
View(air)
R.version
library(spacetime)
data(package="spacetime")
data(air)
library(spacetime)
install.packages('spacetime')
library(spacetime)
data(package="spacetime")
data(air)
plot(air)
str(air)
NA(air)
is.NA(air)
r=0.3
Sigma <- matrix(c(1,r,r,1),2,2)
mvrnorm(n = 10, c(0,0), Sigma)
reguire(MASS)
library(MASS)
mvrnorm(n = 10, c(0,0), Sigma)
r=0.3
Sigma <- matrix(c(1,r,r,1),2,2)
data = mvrnorm(n = 10, c(0,0), Sigma)
m = lm(data[,2] ~ -1+data[,1])
summary(m)
r=0.3
Sigma <- matrix(c(1,r,r,1),2,2)
data = mvrnorm(n = 100000, c(0,0), Sigma)
m = lm(data[,2] ~ -1+data[,1])
summary(m)
m2 = lm(data[,1] ~ -1+data[,2])
summary(m2)
data = mvrnorm(n = 100, c(0,0), Sigma)
plot(data)
r=0.9
Sigma <- matrix(c(1,r,r,1),2,2)
data = mvrnorm(n = 100, c(0,0), Sigma)
plot(data)
r=0.99
Sigma <- matrix(c(1,r,r,1),2,2)
data = mvrnorm(n = 100, c(0,0), Sigma)
plot(data)
m = lm(data[,2] ~ -1+data[,1])
summary(m)
abline(m)
plot(data, xlim=c(-5,5), ylim = c(-5,5) )
m = lm(data[,2] ~ -1+data[,1])
summary(m)
abline(m)
r=0.3
Sigma <- matrix(c(1,r,r,1),2,2)
data = mvrnorm(n = 1000, c(0,0), Sigma)
plot(data, xlim=c(-5,5), ylim = c(-5,5) )
m = lm(data[,2] ~ -1+data[,1])
summary(m)
abline(m)
plot(data[,1] ~ data[,2] , xlim=c(-5,5), ylim = c(-5,5) )
m2 = lm(data[,1] ~ -1+data[,2])
summary(m2)
abline(m2)
r=0.99
Sigma <- matrix(c(1,r,r,1),2,2)
data = mvrnorm(n = 1000, c(0,0), Sigma)
plot(data, xlim=c(-5,5), ylim = c(-5,5) )
m = lm(data[,2] ~ -1+data[,1])
summary(m)
abline(m)
r=0.6
Sigma <- matrix(c(1,r,r,1),2,2)
data = mvrnorm(n = 1000, c(0,0), Sigma)
plot(data, xlim=c(-5,5), ylim = c(-5,5) )
m = lm(data[,2] ~ -1+data[,1])
summary(m)
abline(m)
plot(data[,1] ~ data[,2] , xlim=c(-5,5), ylim = c(-5,5) )
m2 = lm(data[,1] ~ -1+data[,2])
abline(m2)
summary(m2)
b
a = 1
b = 1e-10
b
a+b
1+1
require(gridExtra)
1+1
require(sde)
require(doMC)
require(sandwich)
install.packages(c("sde", "doMC", "sandwich", "ggplot2", "gridExtra"))
setwd("simulation")
setwd("C:\C-epfl results\Simulation of FTS\fARFIMA Li, Robinson, Shang")
setwd("C:/C-epfl results/Simulation of FTS/fARFIMA Li, Robinson, Shang")
source("sim_FARMA.R")
